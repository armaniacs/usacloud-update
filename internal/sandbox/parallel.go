package sandbox

import (
	"context"
	"fmt"
	"os"
	"sync"
	"time"

	"github.com/fatih/color"
	"golang.org/x/time/rate"
)

// JobStatus ã¯ã‚¸ãƒ§ãƒ–ã®å®Ÿè¡ŒçŠ¶æ…‹ã‚’è¡¨ã™
type JobStatus int

const (
	JobPending JobStatus = iota
	JobRunning
	JobCompleted
	JobFailed
	JobCancelled
)

// String ã¯JobStatusã®æ–‡å­—åˆ—è¡¨ç¾ã‚’è¿”ã™
func (j JobStatus) String() string {
	switch j {
	case JobPending:
		return "pending"
	case JobRunning:
		return "running"
	case JobCompleted:
		return "completed"
	case JobFailed:
		return "failed"
	case JobCancelled:
		return "cancelled"
	default:
		return "unknown"
	}
}

// Job ã¯ä¸¦è¡Œå®Ÿè¡Œã™ã‚‹ã‚¸ãƒ§ãƒ–ã‚’è¡¨ã™
type Job struct {
	ID        string                 `json:"id"`
	Command   string                 `json:"command"`
	File      string                 `json:"file,omitempty"`
	Status    JobStatus              `json:"status"`
	Result    *ExecutionResult       `json:"result,omitempty"`
	Error     error                  `json:"error,omitempty"`
	StartTime time.Time              `json:"start_time"`
	EndTime   time.Time              `json:"end_time"`
	Duration  time.Duration          `json:"duration"`
	Metadata  map[string]interface{} `json:"metadata,omitempty"`
}

// ErrorPolicy ã¯ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã®å‡¦ç†æ–¹é‡ã‚’è¡¨ã™
type ErrorPolicy int

const (
	ContinueOnError ErrorPolicy = iota
	StopOnError
	StopOnCriticalError
)

// String ã¯ErrorPolicyã®æ–‡å­—åˆ—è¡¨ç¾ã‚’è¿”ã™
func (e ErrorPolicy) String() string {
	switch e {
	case ContinueOnError:
		return "continue_on_error"
	case StopOnError:
		return "stop_on_error"
	case StopOnCriticalError:
		return "stop_on_critical_error"
	default:
		return "unknown"
	}
}

// ParallelConfig ã¯ä¸¦è¡Œå®Ÿè¡Œã®è¨­å®šã‚’è¡¨ã™
type ParallelConfig struct {
	MaxConcurrency int           `json:"max_concurrency"`
	RateLimit      float64       `json:"rate_limit"` // requests per second
	ErrorPolicy    ErrorPolicy   `json:"error_policy"`
	Timeout        time.Duration `json:"timeout"`
	ShowProgress   bool          `json:"show_progress"`
	Debug          bool          `json:"debug"`
}

// DefaultParallelConfig ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ä¸¦è¡Œå®Ÿè¡Œè¨­å®šã‚’è¿”ã™
func DefaultParallelConfig() *ParallelConfig {
	return &ParallelConfig{
		MaxConcurrency: 5,
		RateLimit:      2.0, // 2 requests per second for Sakura Cloud API
		ErrorPolicy:    ContinueOnError,
		Timeout:        5 * time.Minute,
		ShowProgress:   true,
		Debug:          false,
	}
}

// ParallelExecutor ã¯ä¸¦è¡Œå®Ÿè¡Œã‚’ç®¡ç†ã™ã‚‹
type ParallelExecutor struct {
	executor    ExecutorInterface
	config      *ParallelConfig
	rateLimiter *rate.Limiter
	semaphore   chan struct{}
	ctx         context.Context
	cancel      context.CancelFunc
	wg          sync.WaitGroup
	mu          sync.RWMutex
	jobs        map[string]*Job
	monitor     *ProgressMonitor
}

// NewParallelExecutor ã¯æ–°ã—ã„ParallelExecutorã‚’ä½œæˆã™ã‚‹
func NewParallelExecutor(executor ExecutorInterface, config *ParallelConfig) *ParallelExecutor {
	if config == nil {
		config = DefaultParallelConfig()
	}

	ctx, cancel := context.WithCancel(context.Background())
	if config.Timeout > 0 {
		ctx, cancel = context.WithTimeout(context.Background(), config.Timeout)
	}

	pe := &ParallelExecutor{
		executor:    executor,
		config:      config,
		rateLimiter: rate.NewLimiter(rate.Limit(config.RateLimit), 1),
		semaphore:   make(chan struct{}, config.MaxConcurrency),
		ctx:         ctx,
		cancel:      cancel,
		jobs:        make(map[string]*Job),
		monitor:     NewProgressMonitor(),
	}

	return pe
}

// SubmitJobs ã¯è¤‡æ•°ã®ã‚¸ãƒ§ãƒ–ã‚’ä¸¦è¡Œå®Ÿè¡Œã™ã‚‹
func (pe *ParallelExecutor) SubmitJobs(jobs []*Job) <-chan *Job {
	resultChan := make(chan *Job, len(jobs))

	// ã‚¸ãƒ§ãƒ–ã‚’ãƒãƒƒãƒ—ã«ç™»éŒ²
	pe.mu.Lock()
	for _, job := range jobs {
		if job.ID == "" {
			job.ID = fmt.Sprintf("job_%d", time.Now().UnixNano())
		}
		job.Status = JobPending
		pe.jobs[job.ID] = job
	}
	pe.mu.Unlock()

	// ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒ¢ãƒ‹ã‚¿ãƒ¼åˆæœŸåŒ–
	if pe.config.ShowProgress {
		pe.monitor.Start(len(jobs))
	}

	// å„ã‚¸ãƒ§ãƒ–ã‚’ä¸¦è¡Œå®Ÿè¡Œ
	for _, job := range jobs {
		pe.wg.Add(1)
		go pe.executeJob(job, resultChan)
	}

	// å®Œäº†å¾…ã¡ã® goroutine
	go func() {
		pe.wg.Wait()
		close(resultChan)
		if pe.config.ShowProgress {
			pe.monitor.Finish()
		}
	}()

	return resultChan
}

// ExecuteJob ã¯å˜ä¸€ã®ã‚¸ãƒ§ãƒ–ã‚’å®Ÿè¡Œã™ã‚‹
func (pe *ParallelExecutor) ExecuteJob(job *Job) (*Job, error) {
	jobs := []*Job{job}
	resultChan := pe.SubmitJobs(jobs)

	// çµæœã‚’å¾…æ©Ÿ
	for result := range resultChan {
		if result.ID == job.ID {
			return result, result.Error
		}
	}

	return nil, fmt.Errorf("job execution failed")
}

// executeJob ã¯å†…éƒ¨ã§ã‚¸ãƒ§ãƒ–ã‚’å®Ÿè¡Œã™ã‚‹
func (pe *ParallelExecutor) executeJob(job *Job, resultChan chan<- *Job) {
	defer pe.wg.Done()

	// ã‚»ãƒãƒ•ã‚©ã«ã‚ˆã‚‹ä¸¦è¡Œæ•°åˆ¶é™
	select {
	case pe.semaphore <- struct{}{}:
		defer func() { <-pe.semaphore }()
	case <-pe.ctx.Done():
		job.Status = JobCancelled
		job.Error = pe.ctx.Err()
		pe.sendResult(job, resultChan)
		return
	}

	// ãƒ¬ãƒ¼ãƒˆåˆ¶é™
	if err := pe.rateLimiter.Wait(pe.ctx); err != nil {
		job.Status = JobCancelled
		job.Error = fmt.Errorf("rate limit wait failed: %w", err)
		pe.sendResult(job, resultChan)
		return
	}

	// ã‚¸ãƒ§ãƒ–å®Ÿè¡Œé–‹å§‹
	job.Status = JobRunning
	job.StartTime = time.Now()

	if pe.config.Debug {
		fmt.Fprintf(os.Stderr, color.CyanString("[DEBUG] Starting job %s: %s\n"), job.ID, job.Command)
	}

	// ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ
	result, err := pe.executor.ExecuteCommand(job.Command)
	job.EndTime = time.Now()
	job.Duration = job.EndTime.Sub(job.StartTime)

	if err != nil {
		job.Status = JobFailed
		job.Error = err
		pe.handleJobError(job)
	} else {
		job.Status = JobCompleted
		job.Result = result
	}

	if pe.config.Debug {
		fmt.Fprintf(os.Stderr, color.BlueString("[DEBUG] Completed job %s: status=%s, duration=%v\n"),
			job.ID, job.Status.String(), job.Duration)
	}

	pe.sendResult(job, resultChan)
}

// sendResult ã¯çµæœã‚’é€ä¿¡ã—ã€ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚’æ›´æ–°ã™ã‚‹
func (pe *ParallelExecutor) sendResult(job *Job, resultChan chan<- *Job) {
	// ã‚¸ãƒ§ãƒ–çŠ¶æ…‹ã‚’æ›´æ–°
	pe.mu.Lock()
	pe.jobs[job.ID] = job
	pe.mu.Unlock()

	// ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°
	if pe.config.ShowProgress {
		pe.monitor.Update(job)
	}

	// çµæœé€ä¿¡
	select {
	case resultChan <- job:
	case <-pe.ctx.Done():
		return
	}
}

// handleJobError ã¯ã‚¸ãƒ§ãƒ–ã‚¨ãƒ©ãƒ¼ã‚’å‡¦ç†ã™ã‚‹
func (pe *ParallelExecutor) handleJobError(job *Job) {
	switch pe.config.ErrorPolicy {
	case StopOnError:
		if pe.config.Debug {
			fmt.Fprintf(os.Stderr, color.RedString("[ERROR] Stopping all jobs due to error in job %s: %v\n"),
				job.ID, job.Error)
		}
		pe.cancel() // ã™ã¹ã¦ã®ã‚¸ãƒ§ãƒ–ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«
	case StopOnCriticalError:
		if pe.isCriticalError(job.Error) {
			if pe.config.Debug {
				fmt.Fprintf(os.Stderr, color.RedString("[CRITICAL] Stopping all jobs due to critical error in job %s: %v\n"),
					job.ID, job.Error)
			}
			pe.cancel()
		}
	case ContinueOnError:
		// ç¶šè¡Œï¼ˆä½•ã‚‚ã—ãªã„ï¼‰
		if pe.config.Debug {
			fmt.Fprintf(os.Stderr, color.YellowString("[WARN] Job %s failed, continuing: %v\n"),
				job.ID, job.Error)
		}
	}
}

// isCriticalError ã¯ã‚¨ãƒ©ãƒ¼ãŒé‡å¤§ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹
func (pe *ParallelExecutor) isCriticalError(err error) bool {
	if err == nil {
		return false
	}

	errStr := err.Error()

	// é‡å¤§ãªã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³
	criticalPatterns := []string{
		"authentication",
		"unauthorized",
		"forbidden",
		"api key",
		"token",
		"quota exceeded",
		"service unavailable",
	}

	for _, pattern := range criticalPatterns {
		if contains(errStr, pattern) {
			return true
		}
	}

	return false
}

// contains ã¯å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã›ãšã«æ–‡å­—åˆ—ã«éƒ¨åˆ†æ–‡å­—åˆ—ãŒå«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹
func contains(s, substr string) bool {
	return len(s) >= len(substr) && (s == substr ||
		(len(s) > len(substr) &&
			(s[:len(substr)] == substr ||
				s[len(s)-len(substr):] == substr ||
				containsAt(s, substr))))
}

func containsAt(s, substr string) bool {
	for i := 1; i <= len(s)-len(substr); i++ {
		if s[i:i+len(substr)] == substr {
			return true
		}
	}
	return false
}

// Cancel ã¯ã™ã¹ã¦ã®ã‚¸ãƒ§ãƒ–ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã™ã‚‹
func (pe *ParallelExecutor) Cancel() {
	pe.cancel()
}

// GetJobStatus ã¯æŒ‡å®šã•ã‚ŒãŸã‚¸ãƒ§ãƒ–ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—ã™ã‚‹
func (pe *ParallelExecutor) GetJobStatus(jobID string) (*Job, bool) {
	pe.mu.RLock()
	defer pe.mu.RUnlock()
	job, exists := pe.jobs[jobID]
	return job, exists
}

// GetAllJobs ã¯ã™ã¹ã¦ã®ã‚¸ãƒ§ãƒ–ã‚’å–å¾—ã™ã‚‹
func (pe *ParallelExecutor) GetAllJobs() map[string]*Job {
	pe.mu.RLock()
	defer pe.mu.RUnlock()

	jobs := make(map[string]*Job)
	for id, job := range pe.jobs {
		jobs[id] = job
	}
	return jobs
}

// GetStatistics ã¯å®Ÿè¡Œçµ±è¨ˆã‚’å–å¾—ã™ã‚‹
func (pe *ParallelExecutor) GetStatistics() *ParallelStatistics {
	pe.mu.RLock()
	defer pe.mu.RUnlock()

	stats := &ParallelStatistics{
		TotalJobs:       len(pe.jobs),
		CompletedJobs:   0,
		FailedJobs:      0,
		CancelledJobs:   0,
		RunningJobs:     0,
		PendingJobs:     0,
		TotalDuration:   0,
		AverageDuration: 0,
	}

	var totalDuration time.Duration
	var completedCount int

	for _, job := range pe.jobs {
		switch job.Status {
		case JobCompleted:
			stats.CompletedJobs++
			completedCount++
			totalDuration += job.Duration
		case JobFailed:
			stats.FailedJobs++
		case JobCancelled:
			stats.CancelledJobs++
		case JobRunning:
			stats.RunningJobs++
		case JobPending:
			stats.PendingJobs++
		}
	}

	stats.TotalDuration = totalDuration
	if completedCount > 0 {
		stats.AverageDuration = totalDuration / time.Duration(completedCount)
	}

	return stats
}

// ParallelStatistics ã¯ä¸¦è¡Œå®Ÿè¡Œã®çµ±è¨ˆæƒ…å ±ã‚’ä¿æŒã™ã‚‹
type ParallelStatistics struct {
	TotalJobs       int           `json:"total_jobs"`
	CompletedJobs   int           `json:"completed_jobs"`
	FailedJobs      int           `json:"failed_jobs"`
	CancelledJobs   int           `json:"cancelled_jobs"`
	RunningJobs     int           `json:"running_jobs"`
	PendingJobs     int           `json:"pending_jobs"`
	TotalDuration   time.Duration `json:"total_duration"`
	AverageDuration time.Duration `json:"average_duration"`
}

// ProgressMonitor ã¯é€²æ—è¡¨ç¤ºã‚’ç®¡ç†ã™ã‚‹
type ProgressMonitor struct {
	total      int
	completed  int
	failed     int
	cancelled  int
	mu         sync.RWMutex
	startTime  time.Time
	lastUpdate time.Time
}

// NewProgressMonitor ã¯æ–°ã—ã„ProgressMonitorã‚’ä½œæˆã™ã‚‹
func NewProgressMonitor() *ProgressMonitor {
	return &ProgressMonitor{}
}

// Start ã¯é€²æ—è¡¨ç¤ºã‚’é–‹å§‹ã™ã‚‹
func (pm *ProgressMonitor) Start(total int) {
	pm.mu.Lock()
	defer pm.mu.Unlock()

	pm.total = total
	pm.completed = 0
	pm.failed = 0
	pm.cancelled = 0
	pm.startTime = time.Now()
	pm.lastUpdate = pm.startTime

	fmt.Fprintf(os.Stderr, "\n%s\n", color.HiWhiteString("ğŸš€ ä¸¦è¡Œå®Ÿè¡Œé–‹å§‹"))
	pm.displayProgress()
}

// Update ã¯é€²æ—ã‚’æ›´æ–°ã™ã‚‹
func (pm *ProgressMonitor) Update(job *Job) {
	pm.mu.Lock()
	defer pm.mu.Unlock()

	switch job.Status {
	case JobCompleted:
		pm.completed++
	case JobFailed:
		pm.failed++
	case JobCancelled:
		pm.cancelled++
	}

	// è¡¨ç¤ºæ›´æ–°ï¼ˆ1ç§’ã«1å›ç¨‹åº¦ï¼‰
	now := time.Now()
	if now.Sub(pm.lastUpdate) >= time.Second {
		pm.displayProgress()
		pm.lastUpdate = now
	}
}

// displayProgress ã¯é€²æ—ã‚’è¡¨ç¤ºã™ã‚‹
func (pm *ProgressMonitor) displayProgress() {
	if pm.total == 0 {
		return
	}

	processed := pm.completed + pm.failed + pm.cancelled
	percentage := float64(processed) / float64(pm.total) * 100
	elapsed := time.Since(pm.startTime)

	var eta time.Duration
	if processed > 0 && processed < pm.total {
		avgTime := elapsed / time.Duration(processed)
		remaining := pm.total - processed
		eta = avgTime * time.Duration(remaining)
	}

	// ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
	barWidth := 30
	filledWidth := int(percentage * float64(barWidth) / 100)
	emptyWidth := barWidth - filledWidth

	progressBar := color.GreenString(string(make([]byte, filledWidth))) +
		color.WhiteString(string(make([]byte, emptyWidth)))

	status := fmt.Sprintf("\r[%s] %.1f%% (%d/%d) ",
		progressBar, percentage, processed, pm.total)

	if pm.completed > 0 {
		status += color.GreenString("âœ“%d ", pm.completed)
	}
	if pm.failed > 0 {
		status += color.RedString("âœ—%d ", pm.failed)
	}
	if pm.cancelled > 0 {
		status += color.YellowString("âš %d ", pm.cancelled)
	}

	status += fmt.Sprintf("çµŒé:%v", elapsed.Truncate(time.Second))

	if eta > 0 {
		status += fmt.Sprintf(" æ®‹ã‚Š:%v", eta.Truncate(time.Second))
	}

	fmt.Fprint(os.Stderr, status)
}

// Finish ã¯é€²æ—è¡¨ç¤ºã‚’çµ‚äº†ã™ã‚‹
func (pm *ProgressMonitor) Finish() {
	pm.mu.Lock()
	defer pm.mu.Unlock()

	elapsed := time.Since(pm.startTime)

	fmt.Fprintf(os.Stderr, "\n\n%s\n", color.HiWhiteString("âœ… ä¸¦è¡Œå®Ÿè¡Œå®Œäº†"))
	fmt.Fprintf(os.Stderr, "ç·ã‚¸ãƒ§ãƒ–æ•°:   %d\n", pm.total)
	fmt.Fprintf(os.Stderr, "æˆåŠŸ:         %s\n", color.GreenString("%d", pm.completed))
	fmt.Fprintf(os.Stderr, "å¤±æ•—:         %s\n", color.RedString("%d", pm.failed))
	fmt.Fprintf(os.Stderr, "ã‚­ãƒ£ãƒ³ã‚»ãƒ«:   %s\n", color.YellowString("%d", pm.cancelled))
	fmt.Fprintf(os.Stderr, "å®Ÿè¡Œæ™‚é–“:     %v\n", elapsed.Truncate(time.Second))

	if pm.completed > 0 {
		avgTime := elapsed / time.Duration(pm.completed)
		fmt.Fprintf(os.Stderr, "å¹³å‡å®Ÿè¡Œæ™‚é–“: %v\n", avgTime.Truncate(time.Millisecond))
	}

	fmt.Fprintln(os.Stderr)
}
